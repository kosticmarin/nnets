{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Recurrent neural networks\n",
    "In this exercise we will try a simple experiment with a recurrent neural network. One of the well-known recurrent neural network models is the so called Long short-term memory (LSTM) network. More information on LSTM can be found in the text [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "## 7.1 The MNIST dataset revisited (1)\n",
    "In one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a recurrent neural network to the problem of digits classification. To keep it simple, we will use a simple LSTM network that will be fed with one row of the image at a time. With each new row, it will update its states and give its prediction. What we are interested in is its prediction after the last row i.e. after it has the full information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch #1 0.311\n",
      "Epoch #2 0.4444\n",
      "Epoch #3 0.5713\n",
      "Epoch #4 0.6791\n",
      "Epoch #5 0.7389\n",
      "Epoch #6 0.8217\n",
      "Epoch #7 0.8685\n",
      "Epoch #8 0.8909\n",
      "Epoch #9 0.9106\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "learning_rate=0.01\n",
    "training_epochs_count=10\n",
    "batch_size=128\n",
    "batches_count=int(mnist.train.num_examples/batch_size)\n",
    "display_step=1\n",
    "\n",
    "#we will feed a row at a time to the LSTM and there are 28 rows per image\n",
    "timesteps=28\n",
    "#each row has 28 columns whose values are simultaneously passed to LSTM\n",
    "n_input=28 # MNIST data input (img shape: 28*28)\n",
    "#the number of hidden states in the LSTM\n",
    "n_hidden=128\n",
    "n_classes=10\n",
    "\n",
    "x=tf.placeholder(\"float\", [None, timesteps, n_input])\n",
    "y=tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "#separate the rows separate rows\n",
    "unstacked=tf.unstack(x, timesteps, 1)\n",
    "\n",
    "#prepare the LSTM\n",
    "lstm_cell=rnn.BasicLSTMCell(n_hidden)\n",
    "#feed the rows iteratively to LSTM\n",
    "outputs, states=rnn.static_rnn(lstm_cell, unstacked, dtype=tf.float32)\n",
    "#take the last output (index -1) i.e. the output after the last row and use it for classification\n",
    "logits=tf.layers.dense(outputs[-1], n_classes)\n",
    "y_predicted=tf.nn.softmax(logits)\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_y_predicted=tf.equal(tf.argmax(y_predicted, 1), tf.argmax(y, 1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_y_predicted, tf.float32))\n",
    "\n",
    "#with such a block we don't need to close the session later - it will be closed automatically\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs_count):\n",
    "        for i in range(batches_count):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            session.run(optimizer, feed_dict={x:batch_x.reshape((-1, timesteps, n_input)), y:batch_y})\n",
    "        if ((epoch+1)%display_step==0):\n",
    "            print(\"Epoch #\"+str(epoch+1)+\" \"+str(session.run(accuracy, feed_dict={x: mnist.test.images.reshape((-1, timesteps, n_input)), y: mnist.test.labels})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Study and run the code below.\n",
    "2. Readjust the parameters in order to make the model obtain acuracy above 0.99 on the test set.\n",
    "3. Draw a plot that shows the relation between the number of rows given to the network and its final accuracy on the test set.\n",
    "4. What happens if we use gradient descent instead of Adam?\n",
    "\n",
    "NOTE: when you want to restart the code, first shutdown the kernel e.g. by choosing Kernel in the menu above and then Restart & Clear Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
