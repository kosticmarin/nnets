{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8 Convolutional neural networks\n",
    "The goal of this exercise is to learn the basic stuff about [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN or ConvNet). In the previous exercises the building blocks mostly included simple operations that had some kind of activations functions and the each layer was usually fully connected to the previous one. CNNs take into account the spatial nature of the input data, e.g. an image, and they process it by applying one or more  [kernels](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29). In the case of images, this processing i.e. convolving is also known as filtering. The results of processing the input with a single kernel will be a signle channel, but usually a convolutional layer involves more kernels that then produce more channels. These channels are often called **feature maps** because each kernel is specialized for extraction of a certain kind of features from the input. These feature maps are then combined into a single tensor that can be viewed as an image with multiple channels that can then be passed to further convolutional layers.\n",
    "\n",
    "For example if the input consists of a grayscale image i.e. an image with only one channel and a $5\\times 5$ kernel is applied, the result is a single feature map. The borders of the input image are usuallz padded with zeros in order to ensure that the resulting feature maps has the same number of rows and columns as the input image.\n",
    "\n",
    "If the input consists of a color image i.e. an image with three channels and a $5\\times 5$ kernel is applied, what will actually be applied is an $5\\times 5\\times 3$ kernel that will simultaneously process all three channels and the result will again be a single feature map. However, if e.g. 16 several kernels are applied, then the result will be 16 feature maps. Should they be passed to another convolutional layer, **each** of its kernels would simultaneously process **all** feature maps so their sizes would be e.g. $3\\times 3\\times 16$ or $5\\times 5\\times 16$ where 16 is used to reach all feature maps simultaneously.\n",
    "\n",
    "The convolution is usually followed by applying an element-wise non-linear operation to each of the values in the feature maps. Finally, what offten follows is the summarization i.e. pooling of the information in the feature maps in order to reduce the spatial dimensions and keep only the more important information. A common approach used here is the so called max pooling. It is a non-linear downsampling where the input is divided into a set of non-overlapping rectangles and for each of them only the the maximum value inside of it is kept.\n",
    "\n",
    "![Model of a neuron](cnn_img/max_pooling_2x2.png)\n",
    "<center>Figure 1. Max pooling with $2\\times 2$ rectangles (taken from [Wikipedia](https://en.wikipedia.org/wiki/File:Max_pooling.png)).</center>\n",
    "\n",
    "What usually follows after several convolutional layers is putting the values of all feature maps into a single vector, which is then passed further to fully connected or other kinds of layers.\n",
    "\n",
    "The number of parameters in the convolutional depends on the number of feature maps and the sizes of the kernels. For example is a convolutional layer with 32 kernels of nominal size $3\\times 3$ receives 16 feature maps on its input, it will require $16\\times 3\\times 3\\times 32+32$ where the last 32 covers the kernel biases.\n",
    "\n",
    "\n",
    "## 8.1 The MNIST dataset revisited (2)\n",
    "In one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a convolutional neural network to the problem of digits classification. We will use the following layers to build our model:\n",
    "\n",
    "* [tf.nn.relu](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "* [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)\n",
    "* [tf.layers.max_pooling2d](https://www.tensorflow.org/api_docs/python/tf/layers/max_pooling2d)\n",
    "* [tf.layers.dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense)\n",
    "\n",
    "The [tf.layers.dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense) layer has the same effect as the fully connected layer matrix multiplication that was used in the previous exercise with the MNIST dataset.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Study and run the code below. How is the accuracy compared to the ones obtained in the previous exerises with MNIST?\n",
    "2. Try to change the number and size of convolutional and fully connected layers. What has the greatest impact on the accuracy?\n",
    "3. What happens to the accuracy if another non-linearity is used instead of ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch #1 0.9854\n",
      "Epoch #2 0.9881\n",
      "Epoch #3 0.99\n",
      "Epoch #4 0.9897\n",
      "Epoch #5 0.9899\n"
     ]
    }
   ],
   "source": [
    "#use MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#settings\n",
    "learning_rate=0.001\n",
    "training_epochs_count=5\n",
    "batch_size=100\n",
    "batches_count=int(mnist.train.num_examples/batch_size)\n",
    "display_step=1\n",
    "\n",
    "activation_function=tf.nn.relu\n",
    "optimizer_type=tf.train.AdamOptimizer\n",
    "\n",
    "#architecture\n",
    "input_size=784\n",
    "n_channels_1=32\n",
    "n_channels_2=64\n",
    "n_classes=10\n",
    "n_fully_connected=128\n",
    "kernel_size=5\n",
    "\n",
    "#data input\n",
    "x=tf.placeholder(tf.float32, [None, input_size])\n",
    "\n",
    "#reshaping the input to its image form so that we can apply convolution\n",
    "layer=tf.reshape(x, [-1, 28, 28, 1])\n",
    "y=tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "#first convolutional layer\n",
    "#we will apply n_channels_1 kernels of size kernel_size X kernel_size\n",
    "#we are padding the input in order for the result to have the same number of rows and columns\n",
    "layer=tf.layers.conv2d(layer, n_channels_1, kernel_size, padding=\"SAME\")\n",
    "#applying the non-linearity\n",
    "layer=tf.nn.relu(layer)\n",
    "#now we downsample the feature maps from 28 X 28 to 14 X 14\n",
    "layer=tf.layers.max_pooling2d(layer, 2, 2)\n",
    "\n",
    "#second convolutional layer\n",
    "#we will apply n_channels_2 kernels of size kernel_size X kernel_size\n",
    "layer=tf.layers.conv2d(layer, n_channels_2, kernel_size, padding=\"SAME\")\n",
    "#again, we apply the non-linearity\n",
    "layer=tf.nn.relu(layer)\n",
    "#and max pooling again, now each feature map will be of size 7 X 7\n",
    "layer=tf.layers.max_pooling2d(layer, 2, 2)\n",
    "\n",
    "#we have n_channel_2 maps of size 7 X 7\n",
    "#now reshape them into a single vector\n",
    "layer=tf.reshape(layer, [-1, 7*7*n_channels_2])\n",
    "#a fully connected layer\n",
    "layer=tf.layers.dense(layer, n_fully_connected)\n",
    "#non-linearity\n",
    "layer=tf.nn.relu(layer)\n",
    "\n",
    "#final classification\n",
    "y_predicted=tf.layers.dense(layer, 10)\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_predicted, labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "session=tf.Session();\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "correct_y_predicted=tf.equal(tf.argmax(y_predicted, 1), tf.argmax(y, 1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_y_predicted, tf.float32))\n",
    "\n",
    "for epoch in range(training_epochs_count):\n",
    "    for i in range(batches_count):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        session.run(optimizer, feed_dict={x:batch_x, y:batch_y})\n",
    "    if ((epoch+1)%display_step==0):\n",
    "        print(\"Epoch #\"+str(epoch+1)+\" \"+str(session.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})))\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Image classification\n",
    "Image classification is a challenging computer vision problem with the best known competition being [The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/), which includes the ImageNet dataset with millions of $224\\times 224$ training images. The class names in one of the tasks there can be found [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). One of the most important breakthroughs was when in 2012 the convolutional neural network [AlexNet](https://en.wikipedia.org/wiki/AlexNet) won the first place. Ever since many highly successful convolutional neural networks architectures have been proposed, e.g. [VGG-16](https://arxiv.org/abs/1409.1556), [VGG-19](https://arxiv.org/abs/1409.1556), [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), [Inception](https://arxiv.org/abs/1409.4842), etc. Training such networks requires a lot of time because they have many layers with millions of parameters. In this exercise we are going to experiment with pre-trained models of some of the best known architectures. In order to make things simple, we are going to use [Keras](https://keras.io/), *\"a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.\"* To install Keras, it is enough to type\n",
    "```\n",
    "conda install keras\n",
    "```\n",
    "in your command prompt/terminal. Alternatively, you can type\n",
    "```\n",
    "pip install keras --upgrade\n",
    "```\n",
    "and there be any error, then first type\n",
    "```\n",
    "conda install pip\n",
    "```\n",
    "to refresh your pip and then repeat the first command. Keras already includes APIs to many well-known architectures. Let's first try to classify some images.\n",
    "### 8.2.1 Using pre-trained models\n",
    "Try running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 82s 1us/step\n",
      "Processing image cnn_img/badger.jpg...\n",
      "\tbadger\n",
      "Processing image cnn_img/rabbit.jpg...\n",
      "\twood_rabbit\n",
      "Processing image cnn_img/sundial.jpg...\n",
      "\tsundial\n",
      "Processing image cnn_img/pineapple.jpg...\n",
      "\tpineapple\n",
      "Processing image cnn_img/can.jpg...\n",
      "\tgoblet\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "#choose the architecture\n",
    "#architecture=\"resnet\"\n",
    "#architecture=\"vgg16\"\n",
    "#architecture=\"vgg19\"\n",
    "architecture=\"inceptionv3\"\n",
    "\n",
    "if (architecture==\"resnet\"):\n",
    "    from keras.applications.resnet50 import ResNet50\n",
    "    from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "    model=ResNet50(weights=\"imagenet\")\n",
    "elif (architecture==\"vgg16\"):\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    model=VGG16(weights=\"imagenet\")\n",
    "elif (architecture==\"vgg19\"):\n",
    "    from keras.applications.vgg19 import VGG19\n",
    "    from keras.applications.vgg19 import preprocess_input\n",
    "    model=VGG19(weights=\"imagenet\")\n",
    "elif (architecture==\"inceptionv3\"):\n",
    "    from keras.applications.inception_v3 import InceptionV3\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "    model=InceptionV3(weights=\"imagenet\")\n",
    "    \n",
    "    from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "#images to be classified\n",
    "image_paths=[\"cnn_img/badger.jpg\", \"cnn_img/rabbit.jpg\", \"cnn_img/sundial.jpg\", \"cnn_img/pineapple.jpg\", \"cnn_img/can.jpg\"];\n",
    "for path in image_paths:\n",
    "    #loading the image and rescaling it to fit the size for the imagenet architectures\n",
    "    img=image.load_img(path, target_size=(224, 224))\n",
    "    x=image.img_to_array(img)\n",
    "    x=np.expand_dims(x, axis=0)\n",
    "    x=preprocess_input(x)\n",
    "\n",
    "    print(\"Processing image \"+path+\"...\")\n",
    "    predictions=model.predict(x)\n",
    "    print(\"\\t\"+decode_predictions(predictions, top=1)[0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "1. Is there any significant difference between the results of different architectures?\n",
    "2. Try to classify several other images that you choose on your own. Which cases are problematic?\n",
    "\n",
    "### 8.2.2 Creating your own classifier - pincers vs. scissors\n",
    "Although ImageNet has a lot of classes, sometimes they do not cover some desired cases. Let's assume that we want to tell images with pincers apart from the ones with scissors. Neither pincers nor scissors are among ImageNet classes. Nevertheless, we can still use some parts of the pre-trained models.\n",
    "\n",
    "Various layers of a deep convolutional network have diferent tasks. The ones closest to the original input image usually look for features such as edges and corners i.e. for low-level features. After them there are layers that look for middle-level features such as circular objects, special curves, etc. Next, there are usually fully connected layers that create high-level semantic features by combining the information from the previous layers. These features are then used by the last layer that performs the actual classification. What we can do here is simply to discard the last layer i.e. not to calculate the class of an image, but to extract the values in on of the fully connected layers. This effectively means that we are going to use the network only as an extractor for high-level features that we would hardly be able to engineer on our own. Let's first see which layers can be found in the VGG-16 network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_5\n",
      "conv1\n",
      "bn_conv1\n",
      "activation_144\n",
      "max_pooling2d_6\n",
      "res2a_branch2a\n",
      "bn2a_branch2a\n",
      "activation_145\n",
      "res2a_branch2b\n",
      "bn2a_branch2b\n",
      "activation_146\n",
      "res2a_branch2c\n",
      "res2a_branch1\n",
      "bn2a_branch2c\n",
      "bn2a_branch1\n",
      "add_17\n",
      "activation_147\n",
      "res2b_branch2a\n",
      "bn2b_branch2a\n",
      "activation_148\n",
      "res2b_branch2b\n",
      "bn2b_branch2b\n",
      "activation_149\n",
      "res2b_branch2c\n",
      "bn2b_branch2c\n",
      "add_18\n",
      "activation_150\n",
      "res2c_branch2a\n",
      "bn2c_branch2a\n",
      "activation_151\n",
      "res2c_branch2b\n",
      "bn2c_branch2b\n",
      "activation_152\n",
      "res2c_branch2c\n",
      "bn2c_branch2c\n",
      "add_19\n",
      "activation_153\n",
      "res3a_branch2a\n",
      "bn3a_branch2a\n",
      "activation_154\n",
      "res3a_branch2b\n",
      "bn3a_branch2b\n",
      "activation_155\n",
      "res3a_branch2c\n",
      "res3a_branch1\n",
      "bn3a_branch2c\n",
      "bn3a_branch1\n",
      "add_20\n",
      "activation_156\n",
      "res3b_branch2a\n",
      "bn3b_branch2a\n",
      "activation_157\n",
      "res3b_branch2b\n",
      "bn3b_branch2b\n",
      "activation_158\n",
      "res3b_branch2c\n",
      "bn3b_branch2c\n",
      "add_21\n",
      "activation_159\n",
      "res3c_branch2a\n",
      "bn3c_branch2a\n",
      "activation_160\n",
      "res3c_branch2b\n",
      "bn3c_branch2b\n",
      "activation_161\n",
      "res3c_branch2c\n",
      "bn3c_branch2c\n",
      "add_22\n",
      "activation_162\n",
      "res3d_branch2a\n",
      "bn3d_branch2a\n",
      "activation_163\n",
      "res3d_branch2b\n",
      "bn3d_branch2b\n",
      "activation_164\n",
      "res3d_branch2c\n",
      "bn3d_branch2c\n",
      "add_23\n",
      "activation_165\n",
      "res4a_branch2a\n",
      "bn4a_branch2a\n",
      "activation_166\n",
      "res4a_branch2b\n",
      "bn4a_branch2b\n",
      "activation_167\n",
      "res4a_branch2c\n",
      "res4a_branch1\n",
      "bn4a_branch2c\n",
      "bn4a_branch1\n",
      "add_24\n",
      "activation_168\n",
      "res4b_branch2a\n",
      "bn4b_branch2a\n",
      "activation_169\n",
      "res4b_branch2b\n",
      "bn4b_branch2b\n",
      "activation_170\n",
      "res4b_branch2c\n",
      "bn4b_branch2c\n",
      "add_25\n",
      "activation_171\n",
      "res4c_branch2a\n",
      "bn4c_branch2a\n",
      "activation_172\n",
      "res4c_branch2b\n",
      "bn4c_branch2b\n",
      "activation_173\n",
      "res4c_branch2c\n",
      "bn4c_branch2c\n",
      "add_26\n",
      "activation_174\n",
      "res4d_branch2a\n",
      "bn4d_branch2a\n",
      "activation_175\n",
      "res4d_branch2b\n",
      "bn4d_branch2b\n",
      "activation_176\n",
      "res4d_branch2c\n",
      "bn4d_branch2c\n",
      "add_27\n",
      "activation_177\n",
      "res4e_branch2a\n",
      "bn4e_branch2a\n",
      "activation_178\n",
      "res4e_branch2b\n",
      "bn4e_branch2b\n",
      "activation_179\n",
      "res4e_branch2c\n",
      "bn4e_branch2c\n",
      "add_28\n",
      "activation_180\n",
      "res4f_branch2a\n",
      "bn4f_branch2a\n",
      "activation_181\n",
      "res4f_branch2b\n",
      "bn4f_branch2b\n",
      "activation_182\n",
      "res4f_branch2c\n",
      "bn4f_branch2c\n",
      "add_29\n",
      "activation_183\n",
      "res5a_branch2a\n",
      "bn5a_branch2a\n",
      "activation_184\n",
      "res5a_branch2b\n",
      "bn5a_branch2b\n",
      "activation_185\n",
      "res5a_branch2c\n",
      "res5a_branch1\n",
      "bn5a_branch2c\n",
      "bn5a_branch1\n",
      "add_30\n",
      "activation_186\n",
      "res5b_branch2a\n",
      "bn5b_branch2a\n",
      "activation_187\n",
      "res5b_branch2b\n",
      "bn5b_branch2b\n",
      "activation_188\n",
      "res5b_branch2c\n",
      "bn5b_branch2c\n",
      "add_31\n",
      "activation_189\n",
      "res5c_branch2a\n",
      "bn5c_branch2a\n",
      "activation_190\n",
      "res5c_branch2b\n",
      "bn5c_branch2b\n",
      "activation_191\n",
      "res5c_branch2c\n",
      "bn5c_branch2c\n",
      "add_32\n",
      "activation_192\n",
      "avg_pool\n",
      "flatten_2\n",
      "fc1000\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "base_model=ResNet50(weights=\"imagenet\")\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end you can see fc1 and fc2, which stands for fully connected layers. For example We can extract the values of fc2 by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048)\n"
     ]
    }
   ],
   "source": [
    "#the last layer before the classification layer\n",
    "model=Model(inputs=base_model.input, outputs=base_model.get_layer(base_model.layers[-2].name).output)\n",
    "\n",
    "img_path=\"cnn_img/rabbit.jpg\"\n",
    "img=image.load_img(img_path, target_size=(224, 224))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x, axis=0)\n",
    "x=preprocess_input(x)\n",
    "\n",
    "features=model.predict(x)\n",
    "print(features.shape)\n",
    "feature_layer_size=features.shape[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can now be used as features and that can later be used with another classifier. Let's first extract the features for our pincer and scissors images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training features...\n",
      "\t 1 / 50\n",
      "\t 2 / 50\n",
      "\t 3 / 50\n",
      "\t 4 / 50\n",
      "\t 5 / 50\n",
      "\t 6 / 50\n",
      "\t 7 / 50\n",
      "\t 8 / 50\n",
      "\t 9 / 50\n",
      "\t10 / 50\n",
      "\t11 / 50\n",
      "\t12 / 50\n",
      "\t13 / 50\n",
      "\t14 / 50\n",
      "\t15 / 50\n",
      "\t16 / 50\n",
      "\t17 / 50\n",
      "\t18 / 50\n",
      "\t19 / 50\n",
      "\t20 / 50\n",
      "\t21 / 50\n",
      "\t22 / 50\n",
      "\t23 / 50\n",
      "\t24 / 50\n",
      "\t25 / 50\n",
      "\t26 / 50\n",
      "\t27 / 50\n",
      "\t28 / 50\n",
      "\t29 / 50\n",
      "\t30 / 50\n",
      "\t31 / 50\n",
      "\t32 / 50\n",
      "\t33 / 50\n",
      "\t34 / 50\n",
      "\t35 / 50\n",
      "\t36 / 50\n",
      "\t37 / 50\n",
      "\t38 / 50\n",
      "\t39 / 50\n",
      "\t40 / 50\n",
      "\t41 / 50\n",
      "\t42 / 50\n",
      "\t43 / 50\n",
      "\t44 / 50\n",
      "\t45 / 50\n",
      "\t46 / 50\n",
      "\t47 / 50\n",
      "\t48 / 50\n",
      "\t49 / 50\n",
      "\t50 / 50\n",
      "Creating test features...\n",
      "\t 1 / 50\n",
      "\t 2 / 50\n",
      "\t 3 / 50\n",
      "\t 4 / 50\n",
      "\t 5 / 50\n",
      "\t 6 / 50\n",
      "\t 7 / 50\n",
      "\t 8 / 50\n",
      "\t 9 / 50\n",
      "\t10 / 50\n",
      "\t11 / 50\n",
      "\t12 / 50\n",
      "\t13 / 50\n",
      "\t14 / 50\n",
      "\t15 / 50\n",
      "\t16 / 50\n",
      "\t17 / 50\n",
      "\t18 / 50\n",
      "\t19 / 50\n",
      "\t20 / 50\n",
      "\t21 / 50\n",
      "\t22 / 50\n",
      "\t23 / 50\n",
      "\t24 / 50\n",
      "\t25 / 50\n",
      "\t26 / 50\n",
      "\t27 / 50\n",
      "\t28 / 50\n",
      "\t29 / 50\n",
      "\t30 / 50\n",
      "\t31 / 50\n",
      "\t32 / 50\n",
      "\t33 / 50\n",
      "\t34 / 50\n",
      "\t35 / 50\n",
      "\t36 / 50\n",
      "\t37 / 50\n",
      "\t38 / 50\n",
      "\t39 / 50\n",
      "\t40 / 50\n",
      "\t41 / 50\n",
      "\t42 / 50\n",
      "\t43 / 50\n",
      "\t44 / 50\n",
      "\t45 / 50\n",
      "\t46 / 50\n",
      "\t47 / 50\n",
      "\t48 / 50\n",
      "\t49 / 50\n",
      "\t50 / 50\n"
     ]
    }
   ],
   "source": [
    "def create_numbered_paths(home_dir, n):\n",
    "    return [home_dir+str(i)+\".jpg\" for i in range(n)]\n",
    "\n",
    "def create_paired_numbered_paths(first_home_dir, second_home_dir, n):\n",
    "    image_paths=[]\n",
    "    for p in zip(create_numbered_paths(first_home_dir, n), create_numbered_paths(second_home_dir, n)):\n",
    "        image_paths.extend(p)\n",
    "    return image_paths\n",
    "        \n",
    "def create_features(paths, verbose=True):\n",
    "    n=len(paths)\n",
    "    features=np.zeros((n, feature_layer_size))\n",
    "    for i in range(n):\n",
    "        if (verbose==True):\n",
    "            print(\"\\t%2d / %2d\"%(i+1, n))\n",
    "        img=image.load_img(paths[i], target_size=(224, 224))\n",
    "        img=image.img_to_array(img)\n",
    "        img=np.expand_dims(img, axis=0)\n",
    "        features[i, :]=preprocess_input(model.predict(img))\n",
    "    \n",
    "    return features\n",
    "\n",
    "pincers_dir=\"cnn_img/pincers/\"\n",
    "scissors_dir=\"cnn_img/scissors/\"\n",
    "\n",
    "individual_n=50\n",
    "\n",
    "#combining all image paths\n",
    "image_paths=create_paired_numbered_paths(pincers_dir, scissors_dir, individual_n)\n",
    "\n",
    "#marking their classes\n",
    "image_classes=[]\n",
    "for i in range(individual_n):\n",
    "    #0 stands for the pincer image and 0 stands for the scissors image\n",
    "    image_classes.extend((0, 1))\n",
    "\n",
    "#number of all images\n",
    "n=100\n",
    "#number of training images\n",
    "n_train=50\n",
    "#number of test images\n",
    "n_test=n-n_train\n",
    "\n",
    "print(\"Creating training features...\")\n",
    "#here we will store the features of training images\n",
    "x_train=create_features(image_paths[:n_train])\n",
    "#train classes\n",
    "y_train=np.array(image_classes[:n_train])\n",
    "\n",
    "print(\"Creating test features...\")\n",
    "#here we will store the features of test images\n",
    "x_test=create_features(image_paths[n_train:])\n",
    "#train classes\n",
    "y_test=np.array(image_classes[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that for each image we have its features, we will divide the images into a training and a test set. Then we will use a linear SVM classifier to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def create_svm_classifier(x, y):\n",
    "    #we will use linear SVM\n",
    "    C=1.0\n",
    "    classifier=svm.SVC(kernel=\"linear\", C=C);\n",
    "    classifier.fit(x, y)\n",
    "    return classifier\n",
    "\n",
    "def calculate_accuracy(classifier, x, y):\n",
    "    predicted=classifier.predict(x)\n",
    "    return np.sum(y==predicted)/y.size\n",
    "\n",
    "#training the model\n",
    "classifier=create_svm_classifier(x_train, y_train)\n",
    "\n",
    "#checking the model's accuracy\n",
    "print(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. How has to be the training set for the accuracy to drop significantly?\n",
    "2. Is there any significant gain if more complex SVM models are used?\n",
    "3. What happens if we extract features from another layer, e.g. fc1?\n",
    "\n",
    "### 8.2.1 Creating your own classifier - healthy vs. unhealthy food\n",
    "The previous example was relatively simple because all images were of same size and each of them had a white background, which allowed the extractor to concentrate only on the features of the actual objects. In this example we will use a slightly more complicated case - namely, will will tell images with healthy food apart from the ones with unhealthy food. FIrst let's repeat the same process as we did in the previous example and create the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training features...\n",
      "\t 1 / 100\n",
      "\t 2 / 100\n",
      "\t 3 / 100\n",
      "\t 4 / 100\n",
      "\t 5 / 100\n",
      "\t 6 / 100\n",
      "\t 7 / 100\n",
      "\t 8 / 100\n",
      "\t 9 / 100\n",
      "\t10 / 100\n",
      "\t11 / 100\n",
      "\t12 / 100\n",
      "\t13 / 100\n",
      "\t14 / 100\n",
      "\t15 / 100\n",
      "\t16 / 100\n",
      "\t17 / 100\n",
      "\t18 / 100\n",
      "\t19 / 100\n",
      "\t20 / 100\n",
      "\t21 / 100\n",
      "\t22 / 100\n",
      "\t23 / 100\n",
      "\t24 / 100\n",
      "\t25 / 100\n",
      "\t26 / 100\n",
      "\t27 / 100\n",
      "\t28 / 100\n",
      "\t29 / 100\n",
      "\t30 / 100\n",
      "\t31 / 100\n",
      "\t32 / 100\n",
      "\t33 / 100\n",
      "\t34 / 100\n",
      "\t35 / 100\n",
      "\t36 / 100\n",
      "\t37 / 100\n",
      "\t38 / 100\n",
      "\t39 / 100\n",
      "\t40 / 100\n",
      "\t41 / 100\n",
      "\t42 / 100\n",
      "\t43 / 100\n",
      "\t44 / 100\n",
      "\t45 / 100\n",
      "\t46 / 100\n",
      "\t47 / 100\n",
      "\t48 / 100\n",
      "\t49 / 100\n",
      "\t50 / 100\n",
      "\t51 / 100\n",
      "\t52 / 100\n",
      "\t53 / 100\n",
      "\t54 / 100\n",
      "\t55 / 100\n",
      "\t56 / 100\n",
      "\t57 / 100\n",
      "\t58 / 100\n",
      "\t59 / 100\n",
      "\t60 / 100\n",
      "\t61 / 100\n",
      "\t62 / 100\n",
      "\t63 / 100\n",
      "\t64 / 100\n",
      "\t65 / 100\n",
      "\t66 / 100\n",
      "\t67 / 100\n",
      "\t68 / 100\n",
      "\t69 / 100\n",
      "\t70 / 100\n",
      "\t71 / 100\n",
      "\t72 / 100\n",
      "\t73 / 100\n",
      "\t74 / 100\n",
      "\t75 / 100\n",
      "\t76 / 100\n",
      "\t77 / 100\n",
      "\t78 / 100\n",
      "\t79 / 100\n",
      "\t80 / 100\n",
      "\t81 / 100\n",
      "\t82 / 100\n",
      "\t83 / 100\n",
      "\t84 / 100\n",
      "\t85 / 100\n",
      "\t86 / 100\n",
      "\t87 / 100\n",
      "\t88 / 100\n",
      "\t89 / 100\n",
      "\t90 / 100\n",
      "\t91 / 100\n",
      "\t92 / 100\n",
      "\t93 / 100\n",
      "\t94 / 100\n",
      "\t95 / 100\n",
      "\t96 / 100\n",
      "\t97 / 100\n",
      "\t98 / 100\n",
      "\t99 / 100\n",
      "\t100 / 100\n",
      "Creating test features...\n",
      "\t 1 / 100\n",
      "\t 2 / 100\n",
      "\t 3 / 100\n",
      "\t 4 / 100\n",
      "\t 5 / 100\n",
      "\t 6 / 100\n",
      "\t 7 / 100\n",
      "\t 8 / 100\n",
      "\t 9 / 100\n",
      "\t10 / 100\n",
      "\t11 / 100\n",
      "\t12 / 100\n",
      "\t13 / 100\n",
      "\t14 / 100\n",
      "\t15 / 100\n",
      "\t16 / 100\n",
      "\t17 / 100\n",
      "\t18 / 100\n",
      "\t19 / 100\n",
      "\t20 / 100\n",
      "\t21 / 100\n",
      "\t22 / 100\n",
      "\t23 / 100\n",
      "\t24 / 100\n",
      "\t25 / 100\n",
      "\t26 / 100\n",
      "\t27 / 100\n",
      "\t28 / 100\n",
      "\t29 / 100\n",
      "\t30 / 100\n",
      "\t31 / 100\n",
      "\t32 / 100\n",
      "\t33 / 100\n",
      "\t34 / 100\n",
      "\t35 / 100\n",
      "\t36 / 100\n",
      "\t37 / 100\n",
      "\t38 / 100\n",
      "\t39 / 100\n",
      "\t40 / 100\n",
      "\t41 / 100\n",
      "\t42 / 100\n",
      "\t43 / 100\n",
      "\t44 / 100\n",
      "\t45 / 100\n",
      "\t46 / 100\n",
      "\t47 / 100\n",
      "\t48 / 100\n",
      "\t49 / 100\n",
      "\t50 / 100\n",
      "\t51 / 100\n",
      "\t52 / 100\n",
      "\t53 / 100\n",
      "\t54 / 100\n",
      "\t55 / 100\n",
      "\t56 / 100\n",
      "\t57 / 100\n",
      "\t58 / 100\n",
      "\t59 / 100\n",
      "\t60 / 100\n",
      "\t61 / 100\n",
      "\t62 / 100\n",
      "\t63 / 100\n",
      "\t64 / 100\n",
      "\t65 / 100\n",
      "\t66 / 100\n",
      "\t67 / 100\n",
      "\t68 / 100\n",
      "\t69 / 100\n",
      "\t70 / 100\n",
      "\t71 / 100\n",
      "\t72 / 100\n",
      "\t73 / 100\n",
      "\t74 / 100\n",
      "\t75 / 100\n",
      "\t76 / 100\n",
      "\t77 / 100\n",
      "\t78 / 100\n",
      "\t79 / 100\n",
      "\t80 / 100\n",
      "\t81 / 100\n",
      "\t82 / 100\n",
      "\t83 / 100\n",
      "\t84 / 100\n",
      "\t85 / 100\n",
      "\t86 / 100\n",
      "\t87 / 100\n",
      "\t88 / 100\n",
      "\t89 / 100\n",
      "\t90 / 100\n",
      "\t91 / 100\n",
      "\t92 / 100\n",
      "\t93 / 100\n",
      "\t94 / 100\n",
      "\t95 / 100\n",
      "\t96 / 100\n",
      "\t97 / 100\n",
      "\t98 / 100\n",
      "\t99 / 100\n",
      "\t100 / 100\n"
     ]
    }
   ],
   "source": [
    "healthy_dir=\"cnn_img/healthy/\"\n",
    "unhealthy_dir=\"cnn_img/unhealthy/\"\n",
    "\n",
    "individual_n=100\n",
    "\n",
    "#combining all image paths\n",
    "image_paths=create_paired_numbered_paths(healthy_dir, unhealthy_dir, individual_n)\n",
    "\n",
    "#marking their classes\n",
    "image_classes=[]\n",
    "for i in range(individual_n):\n",
    "    #0 stands for the pincer image and 0 stands for the scissors image\n",
    "    image_classes.extend((0, 1))\n",
    "\n",
    "#number of all images\n",
    "n=200\n",
    "#number of training images\n",
    "n_train=100\n",
    "#number of test images\n",
    "n_test=n-n_train\n",
    "\n",
    "print(\"Creating training features...\")\n",
    "#here we will store the features of training images\n",
    "x_train=create_features(image_paths[:n_train])\n",
    "#train classes\n",
    "y_train=np.array(image_classes[:n_train])\n",
    "\n",
    "print(\"Creating test features...\")\n",
    "#here we will store the features of test images\n",
    "x_test=create_features(image_paths[n_train:])\n",
    "#train classes\n",
    "y_test=np.array(image_classes[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a model and test its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.00%\n"
     ]
    }
   ],
   "source": [
    "classifier=create_svm_classifier(x_train, y_train)\n",
    "print(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "1. What is the effect of choosing some other layers for feature extraction?\n",
    "2. Try the whole food classification with another network as feature extractor.\n",
    "3. What kind of test images are problematic?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
